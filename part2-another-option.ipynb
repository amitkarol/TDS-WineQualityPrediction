{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Error Analysis Conclusions & Work Plan\n",
    "\n",
    "#### **Conclusions from Error Analysis**\n",
    "Based on the initial error analysis, the following factors were identified as the main causes of errors in the baseline model:\n",
    "\n",
    "1. **Skewed Distributions**\n",
    "   - Features like `residual sugar` and `total sulfur dioxide` have highly skewed distributions.\n",
    "   - The skewness introduces bias in the model’s learning process, leading to poor generalization on extreme values and outliers.\n",
    "\n",
    "2. **Underrepresentation of Key Patterns**\n",
    "   - Certain combinations of features (e.g., very low `residual sugar` with high `total sulfur dioxide`) are rare in the dataset, causing the model to underperform on these edge cases.\n",
    "\n",
    "3. **Feature Redundancy**\n",
    "   - High correlation between `free sulfur dioxide` and `total sulfur dioxide` indicates redundancy. This may confuse the model and lead to inefficiencies in learning.\n",
    "\n",
    "4. **Bias-Variance Tradeoff**\n",
    "   - The current model may overfit dominant patterns while underfitting on edge cases, likely due to suboptimal hyperparameters or insufficient regularization.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Work Plan for Addressing Errors**\n",
    "To address the issues identified, the following steps will be implemented:\n",
    "\n",
    "1. **Handling Skewed Distributions**\n",
    "   - Apply log transformations to skewed features (`residual sugar`, `total sulfur dioxide`) to stabilize variance and reduce skewness.\n",
    "   - Use visualizations like histograms to confirm the effect of the transformation.\n",
    "\n",
    "2. **Improving Representation of Key Patterns**\n",
    "   - Perform data augmentation techniques like SMOTE (Synthetic Minority Oversampling Technique) to increase representation for underrepresented feature combinations.\n",
    "   - Stratify the training data to ensure even representation of different feature ranges during training.\n",
    "\n",
    "3. **Reducing Feature Redundancy**\n",
    "   - Create a new composite feature: the sulfur dioxide ratio (`free sulfur dioxide / total sulfur dioxide`) to capture relationships between the redundant features.\n",
    "   - Remove the individual redundant features after validating the new feature’s utility.\n",
    "\n",
    "4. **Hyperparameter Tuning**\n",
    "   - Optimize key hyperparameters such as `max_depth`, `learning_rate`, `n_estimators`, and `min_child_weight` using Grid Search or Randomized Search.\n",
    "   - Adjust regularization parameters (e.g., `lambda`, `alpha`) to balance bias and variance effectively.\n",
    "\n",
    "5. **Outlier Handling**\n",
    "   - Detect and mitigate outliers using techniques like IQR-based capping or robust scaling methods to reduce their influence on the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Improving Model Performance\n",
    "\n",
    "#### **Identifying Weaknesses in the Baseline Model**\n",
    "The baseline model exhibits the following weaknesses:\n",
    "1. **Sensitivity to Skewed Features**:\n",
    "   - Skewed distributions in `residual sugar` and `total sulfur dioxide` negatively impact the model’s ability to generalize.\n",
    "2. **Redundant and Low-Importance Features**:\n",
    "   - Redundant features (e.g., `free sulfur dioxide`, `total sulfur dioxide`) dilute the model’s focus.\n",
    "   - Features with low correlation to the target (e.g., `pH`) may introduce noise.\n",
    "3. **Lack of Robust Outlier Handling**:\n",
    "   - Extreme values adversely affect the model’s predictions, particularly on edge cases.\n",
    "4. **Limited Hyperparameter Optimization**:\n",
    "   - The current hyperparameter configuration may not adequately balance bias and variance.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Steps to Improve Performance**\n",
    "\n",
    "1. ### **Hyperparameter Tuning**\n",
    "   - Use a systematic approach like Grid Search or Randomized Search to find optimal hyperparameters:\n",
    "     - `max_depth`: Increase to allow the model to capture more complex patterns.\n",
    "     - `n_estimators`: Experiment with higher values for better ensemble performance.\n",
    "     - `learning_rate`: Lower the learning rate while increasing `n_estimators` for a more gradual learning process.\n",
    "     - `min_child_weight`: Optimize to control the model’s sensitivity to small sample sizes.\n",
    "   - Explore regularization parameters (`lambda`, `alpha`) to mitigate overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 729 candidates, totalling 3645 fits\n",
      "Best Parameters: {'alpha': 0.5, 'lambda': 1, 'learning_rate': 0.1, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 300}\n",
      "Best MSE: 0.4032341796945539\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('wine-quality-white-and-red.csv')\n",
    "\n",
    "# Check and encode categorical columns\n",
    "if 'type' in data.columns:\n",
    "    label_encoder = LabelEncoder()\n",
    "    data['type'] = label_encoder.fit_transform(data['type'])  # Convert 'type' to numerical\n",
    "\n",
    "# Prepare features (X) and target (y)\n",
    "X = data.drop(columns=['quality'])  # Features\n",
    "y = data['quality']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model with experimental categorical support\n",
    "xgb_model = XGBRegressor(\n",
    "    random_state=42,\n",
    "    objective='reg:squarederror'\n",
    ")\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],           # Control the complexity of the model\n",
    "    'n_estimators': [100, 200, 300],  # Number of trees in the ensemble\n",
    "    'learning_rate': [0.01, 0.1, 0.2], # Step size for weight updates\n",
    "    'min_child_weight': [1, 3, 5],    # Minimum sum of instance weight in a child\n",
    "    'lambda': [1, 2, 5],              # L2 regularization term\n",
    "    'alpha': [0, 0.1, 0.5]            # L1 regularization term\n",
    "}\n",
    "\n",
    "# Perform Grid Search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error', # Use MSE as evaluation metric\n",
    "    cv=5,                             # 5-fold cross-validation\n",
    "    verbose=1,                        # Output progress\n",
    "    n_jobs=-1                         # Use all available processors\n",
    ")\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = -grid_search.best_score_  # Convert back to positive MSE\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best MSE:\", best_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **2. Feature Engineering**\n",
    "\n",
    "To enhance the model's performance, the following feature engineering techniques are applied:\n",
    "\n",
    "**Transform Skewed Features**\n",
    "- Apply log transformations to features like `residual sugar` and `total sulfur dioxide` to reduce skewness and stabilize variance.\n",
    "- Log transformations help to make the data distribution closer to normal, which is beneficial for the model's learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transformation of skewed features\n",
    "import numpy as np\n",
    "\n",
    "X_train['residual sugar'] = np.log1p(X_train['residual sugar'])\n",
    "X_train['total sulfur dioxide'] = np.log1p(X_train['total sulfur dioxide'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Composite Features**\n",
    "\n",
    "Introduce a new feature: the **sulfur dioxide ratio**, calculated as:\n",
    "\n",
    "This composite feature replaces the redundant features (`free sulfur dioxide` and `total sulfur dioxide`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sulfur dioxide ratio feature\n",
    "X_train['sulfur_dioxide_ratio'] = X_train['free sulfur dioxide'] / (X_train['total sulfur dioxide'] + 1e-6)\n",
    "\n",
    "# Drop redundant features\n",
    "X_train.drop(columns=['free sulfur dioxide', 'total sulfur dioxide'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Remove Low-Importance Features**\n",
    "\n",
    "Use feature importance scores from the model to identify and remove features contributing minimally to predictions.\n",
    "\n",
    "This step simplifies the model and reduces noise from irrelevant features, ensuring the model focuses on the most impactful variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify alignment between features and feature importances\n",
    "if len(X_train.columns) != len(feature_importances):\n",
    "    print(\"Mismatch detected. Retraining the model to align features...\")\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    feature_importances = xgb_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Drop features with importance below a certain threshold (e.g., 0.01)\n",
    "low_importance_features = feature_importances_df[feature_importances_df['Importance'] < 0.01]['Feature']\n",
    "X_train.drop(columns=low_importance_features, inplace=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

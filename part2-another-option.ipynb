{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Error Analysis Conclusions & Work Plan\n",
    "\n",
    "#### **Conclusions from Error Analysis**\n",
    "Based on the initial error analysis, the following factors were identified as the main causes of errors in the baseline model:\n",
    "\n",
    "1. **Skewed Distributions**\n",
    "   - Features like `residual sugar` and `total sulfur dioxide` have highly skewed distributions.\n",
    "   - The skewness introduces bias in the model’s learning process, leading to poor generalization on extreme values and outliers.\n",
    "\n",
    "2. **Underrepresentation of Key Patterns**\n",
    "   - Certain combinations of features (e.g., very low `residual sugar` with high `total sulfur dioxide`) are rare in the dataset, causing the model to underperform on these edge cases.\n",
    "\n",
    "3. **Feature Redundancy**\n",
    "   - High correlation between `free sulfur dioxide` and `total sulfur dioxide` indicates redundancy. This may confuse the model and lead to inefficiencies in learning.\n",
    "\n",
    "4. **Bias-Variance Tradeoff**\n",
    "   - The current model may overfit dominant patterns while underfitting on edge cases, likely due to suboptimal hyperparameters or insufficient regularization.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Work Plan for Addressing Errors**\n",
    "To address the issues identified, the following steps will be implemented:\n",
    "\n",
    "1. **Handling Skewed Distributions**\n",
    "   - Apply log transformations to skewed features (`residual sugar`, `total sulfur dioxide`) to stabilize variance and reduce skewness.\n",
    "   - Use visualizations like histograms to confirm the effect of the transformation.\n",
    "\n",
    "2. **Improving Representation of Key Patterns**\n",
    "   - Perform data augmentation techniques like SMOTE (Synthetic Minority Oversampling Technique) to increase representation for underrepresented feature combinations.\n",
    "   - Stratify the training data to ensure even representation of different feature ranges during training.\n",
    "\n",
    "3. **Reducing Feature Redundancy**\n",
    "   - Create a new composite feature: the sulfur dioxide ratio (`free sulfur dioxide / total sulfur dioxide`) to capture relationships between the redundant features.\n",
    "   - Remove the individual redundant features after validating the new feature’s utility.\n",
    "\n",
    "4. **Hyperparameter Tuning**\n",
    "   - Optimize key hyperparameters such as `max_depth`, `learning_rate`, `n_estimators`, and `min_child_weight` using Grid Search or Randomized Search.\n",
    "   - Adjust regularization parameters (e.g., `lambda`, `alpha`) to balance bias and variance effectively.\n",
    "\n",
    "5. **Outlier Handling**\n",
    "   - Detect and mitigate outliers using techniques like IQR-based capping or robust scaling methods to reduce their influence on the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Improving Model Performance\n",
    "\n",
    "#### **Identifying Weaknesses in the Baseline Model**\n",
    "The baseline model exhibits the following weaknesses:\n",
    "1. **Sensitivity to Skewed Features**:\n",
    "   - Skewed distributions in `residual sugar` and `total sulfur dioxide` negatively impact the model’s ability to generalize.\n",
    "2. **Redundant and Low-Importance Features**:\n",
    "   - Redundant features (e.g., `free sulfur dioxide`, `total sulfur dioxide`) dilute the model’s focus.\n",
    "   - Features with low correlation to the target (e.g., `pH`) may introduce noise.\n",
    "3. **Lack of Robust Outlier Handling**:\n",
    "   - Extreme values adversely affect the model’s predictions, particularly on edge cases.\n",
    "4. **Limited Hyperparameter Optimization**:\n",
    "   - The current hyperparameter configuration may not adequately balance bias and variance.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Steps to Improve Performance**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hyperparameter Tuning**\n",
    "\n",
    "To optimize the performance of our model, we conducted a grid search over a range of hyperparameters for the XGBoost Regressor. This process involved training the model using various combinations of the following parameters:\n",
    "\n",
    "1. **`max_depth`**: Controls the complexity of the model by setting the maximum depth of the trees.\n",
    "2. **`n_estimators`**: Determines the number of trees in the model.\n",
    "3. **`learning_rate`**: Controls the step size during weight updates.\n",
    "4. **`min_child_weight`**: Ensures splits have a sufficient number of data points.\n",
    "5. **`lambda`**: L2 regularization term to reduce overfitting.\n",
    "6. **`alpha`**: L1 regularization term to create sparse models.\n",
    "\n",
    "The grid search used 5-fold cross-validation and evaluated each parameter combination based on the negative mean squared error (MSE). This approach ensured that the model was optimized for generalization on unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Results of Hyperparameter Tuning**\n",
    "\n",
    "After running the grid search, the following parameters were found to be optimal for our dataset:\n",
    "\n",
    "- **`alpha`**: \\( 0.5 \\)\n",
    "- **`lambda`**: \\( 1 \\)\n",
    "- **`learning_rate`**: \\( 0.1 \\)\n",
    "- **`max_depth`**: \\( 7 \\)\n",
    "- **`min_child_weight`**: \\( 1 \\)\n",
    "- **`n_estimators`**: \\( 300 \\)\n",
    "\n",
    "The best mean squared error (MSE) achieved with these parameters was:\n",
    "\n",
    "- **best MSE** \\(0.4032\\)\n",
    "\n",
    "---\n",
    "\n",
    "### **Next Steps**\n",
    "\n",
    "Using these optimized parameters, we will:\n",
    "1. Train the model to evaluate its performance on the test set.\n",
    "2. Use the results as a baseline for further improvements such as feature engineering and validation.\n",
    "3. Compare the metrics achieved after optimization with the baseline model to measure the improvement.\n",
    "\n",
    "This information will guide the subsequent stages of model improvement and analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **2. Feature Engineering**\n",
    "\n",
    "To enhance the model's performance, the following feature engineering techniques are applied:\n",
    "\n",
    "**Transform Skewed Features**\n",
    "- Apply log transformations to features like `residual sugar` and `total sulfur dioxide` to reduce skewness and stabilize variance.\n",
    "- Log transformations help to make the data distribution closer to normal, which is beneficial for the model's learning process.\n",
    "\n",
    "**Create Composite Features**\n",
    "\n",
    "Introduce a new feature: the **sulfur dioxide ratio**, calculated as:\n",
    "\n",
    "This composite feature replaces the redundant features (`free sulfur dioxide` and `total sulfur dioxide`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, PolynomialFeatures\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('wine-quality-white-and-red.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical column 'type' if it exists\n",
    "if 'type' in data.columns:\n",
    "    label_encoder = LabelEncoder()\n",
    "    data['type'] = label_encoder.fit_transform(data['type'])  # Convert 'type' to numerical\n",
    "\n",
    "# Prepare features (X) and target (y)\n",
    "X = data.drop(columns=['quality'])  # Features\n",
    "y = data['quality']  # Target variable\n",
    "\n",
    "# Handle imbalanced target using SMOTE with k_neighbors=3\n",
    "smote = SMOTE(random_state=42, k_neighbors=3)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "for df in [X_train, X_test]:\n",
    "    # Create sulfur dioxide ratio feature\n",
    "    df['sulfur_dioxide_ratio'] = df['free sulfur dioxide'] / (df['total sulfur dioxide'] + 1e-6)\n",
    "    # Apply log transformation to skewed features\n",
    "    for col in ['residual sugar', 'total sulfur dioxide']:\n",
    "        if col in df.columns:\n",
    "            df[col] = np.log1p(df[col])\n",
    "    # Drop redundant features\n",
    "    df.drop(columns=['free sulfur dioxide', 'total sulfur dioxide'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Remove Low-Importance Features**\n",
    "\n",
    "Use feature importance scores from the model to identify and remove features contributing minimally to predictions.\n",
    "\n",
    "This step simplifies the model and reduces noise from irrelevant features, ensuring the model focuses on the most impactful variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify alignment between features and feature importances\n",
    "xgb_model = XGBRegressor(random_state=42, objective='reg:squarederror')  # Initialize model\n",
    "xgb_model.fit(X_train, y_train)  # Train the model\n",
    "feature_importances = xgb_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for feature importances\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Drop features with importance below a certain threshold\n",
    "low_importance_features = feature_importances_df[feature_importances_df['Importance'] < 0.001]['Feature']\n",
    "X_train.drop(columns=low_importance_features, inplace=True)\n",
    "X_test.drop(columns=low_importance_features, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Metrics:\n",
      "MAE: 0.3651, MSE: 0.2957, RMSE: 0.5438, R^2: 0.9252\n",
      "Cross-Validated MSE: 0.3008\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Add polynomial features\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "poly_feature_names = poly.get_feature_names_out(X_train.columns)\n",
    "\n",
    "# Train the model with known best parameters\n",
    "best_model = XGBRegressor(\n",
    "    random_state=42,\n",
    "    objective='reg:squarederror',\n",
    "    alpha=1.0,  # Best L1 regularization\n",
    "    reg_lambda=10,  # Best L2 regularization\n",
    "    learning_rate=0.05,  # Best learning rate\n",
    "    max_depth=5,  # Best tree depth\n",
    "    min_child_weight=3,  # Best child weight\n",
    "    n_estimators=500  # Best number of trees\n",
    ")\n",
    "\n",
    "# Fit the model on the training data\n",
    "best_model.fit(X_train_poly, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_test = best_model.predict(X_test_poly)\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Testing Metrics:\")\n",
    "print(f\"MAE: {mae_test:.4f}, MSE: {mse_test:.4f}, RMSE: {rmse_test:.4f}, R^2: {r2_test:.4f}\")\n",
    "\n",
    "# Cross-Validation\n",
    "cv_scores = cross_val_score(best_model, X_train_poly, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "mean_cv_mse = -np.mean(cv_scores)\n",
    "print(f\"Cross-Validated MSE: {mean_cv_mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular Data Science – Final Research Project\n",
    "### Overview\n",
    "This notebook presents an automated feature engineering approach to improve model performance across multiple datasets. We compare a baseline model trained on the original features with an enhanced model that includes automatically generated features.\n",
    "### Approach\n",
    "1. Baseline Model: Train and evaluate a simple model on the raw dataset.\n",
    "2. Feature Engineering: Automatically generate, filter, and rank new features.\n",
    "3. Enhanced Model: Train and evaluate a model with the engineered features.\n",
    "4. Comparison: Compare baseline vs. enhanced model performance using statistical tests.\n",
    "### Datasets Used\n",
    "We evaluate our approach on four different datasets:\n",
    "- Cancer Patient Data (Classification)\n",
    "- Amsterdam Rental Prices (Regression)\n",
    "- Student Performance Factors (Classification)\n",
    "- Life expectancy (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,r2_score, mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import ttest_rel\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from scipy.stats import wilcoxon\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "sys.path.append(os.path.abspath(\"feature_engineering\"))\n",
    "from feature_generator import SemiAutomatedFeatureEngineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 1: Cancer Patient Data (Classification)\n",
    "Goal: Predict cancer severity level based on patient attributes.\n",
    "\n",
    "- Type: Classification (Target: Level)\n",
    "- Baseline Model: RandomForestClassifier\n",
    "- Enhanced Model: Random Forest with Engineered Features\n",
    "- Comparison Metrics: Accuracy, Precision, Recall, F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline Model Performance:\n",
      "{'Accuracy': 0.9354838709677419, 'Precision': 0.946236559139785, 'Recall': 0.9354838709677419, 'F1 Score': 0.9319648093841643}\n",
      "\n",
      "Running Feature Engineering...\n",
      "[FeatureEngineering] Starting pipeline...\n",
      "[FeatureEngineering] Generating new features...\n",
      "[FeatureEngineering] Added 1012 new features.\n",
      "Sample new features: ['Balanced Diet_div_Swallowing Difficulty', 'Balanced Diet_times_Clubbing of Finger Nails', 'Balanced Diet_times_Dry Cough', 'Dust Allergy_minus_Clubbing of Finger Nails', 'Chest Pain_minus_Shortness of Breath', 'Frequent Cold_times_Snoring', 'Gender_plus_Passive Smoker', 'Chest Pain_plus_Wheezing', 'chronic Lung Disease_minus_Chest Pain', 'Obesity_times_Clubbing of Finger Nails']\n",
      "[FeatureEngineering] Filtering features based on correlation & variance...\n",
      "[FeatureEngineering] Dropping 114\n",
      "[FeatureEngineering] 900 new features were retained after filtering.\n",
      "[FeatureEngineering] Checking final set of features for importance...\n",
      "[FeatureEngineering] Computing feature importance via SHAP & feature_importances_...\n",
      "\n",
      "[FeatureEngineering] Important Features (Combined SHAP + Permutation):\n",
      " Alcohol use      0.000347\n",
      "Air Pollution    0.000286\n",
      "Gender           0.000278\n",
      "dtype: float64\n",
      "[FeatureEngineering] Training & evaluating final model with new features...\n",
      "\n",
      "[FeatureEngineering] Model Performance with Engineered Features: {'Accuracy': 0.967741935483871, 'Precision': 0.969758064516129, 'Recall': 0.967741935483871, 'F1 Score': 0.9667959511872103}\n",
      "\n",
      "[FeatureEngineering] Summary:\n",
      "- 1012 new features generated\n",
      "- 922 total features after filtering\n",
      "- 900 new features retained\n",
      "\n",
      "Comparison Between Baseline and Enhanced Model:\n",
      "Baseline Model: {'Accuracy': 0.9354838709677419, 'Precision': 0.946236559139785, 'Recall': 0.9354838709677419, 'F1 Score': 0.9319648093841643}\n",
      "Enhanced Model: {'Accuracy': 0.967741935483871, 'Precision': 0.969758064516129, 'Recall': 0.967741935483871, 'F1 Score': 0.9667959511872103}\n",
      "\n",
      "Paired T-Test: t=-12.4158, p=0.0011\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# 1. Load & Preprocess the Data\n",
    "###############################################################################\n",
    "dataset_path = os.path.abspath(\"../Data/cancer_patient.csv\")\n",
    "df_cancer = pd.read_csv(dataset_path)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "if 'index' in df_cancer.columns:\n",
    "    df_cancer.drop(columns=['index'], inplace=True)\n",
    "if 'Patient Id' in df_cancer.columns:\n",
    "    df_cancer.drop(columns=['Patient Id'], inplace=True)\n",
    "\n",
    "df_cancer.drop_duplicates(inplace=True)\n",
    "\n",
    "# Encode the target column\n",
    "label_encoder = LabelEncoder()\n",
    "df_cancer['Level'] = label_encoder.fit_transform(df_cancer['Level'])\n",
    "\n",
    "# Save the cleaned dataset\n",
    "cleaned_path = os.path.abspath(\"../Data/cancer_patient_clean.csv\")\n",
    "df_cancer.to_csv(cleaned_path, index=False)\n",
    "\n",
    "###############################################################################\n",
    "# 2. Baseline Model Training\n",
    "###############################################################################\n",
    "X = df_cancer.drop(columns=['Level'])\n",
    "y = df_cancer['Level']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Simple baseline model\n",
    "baseline_model = RandomForestClassifier(\n",
    "    n_estimators=50, max_depth=5, min_samples_split=10, random_state=42\n",
    ")\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate baseline model\n",
    "y_pred_baseline = baseline_model.predict(X_test)\n",
    "baseline_results = {\n",
    "    \"Accuracy\": accuracy_score(y_test, y_pred_baseline),\n",
    "    \"Precision\": precision_score(y_test, y_pred_baseline, average='weighted'),\n",
    "    \"Recall\": recall_score(y_test, y_pred_baseline, average='weighted'),\n",
    "    \"F1 Score\": f1_score(y_test, y_pred_baseline, average='weighted')\n",
    "}\n",
    "\n",
    "print(\"\\nBaseline Model Performance:\")\n",
    "print(baseline_results)\n",
    "\n",
    "###############################################################################\n",
    "# 3. Feature Engineering (SemiAutomatedFeatureEngineering)\n",
    "###############################################################################\n",
    "print(\"\\nRunning Feature Engineering...\")\n",
    "feature_engineer = SemiAutomatedFeatureEngineering(\n",
    "    df_cancer.copy(), \n",
    "    target_column=\"Level\",\n",
    "    task=\"classification\",\n",
    "    correlation_threshold=0.05,\n",
    "    variance_threshold=0.01\n",
    ")\n",
    "enhanced_results = feature_engineer.run_pipeline()\n",
    "\n",
    "###############################################################################\n",
    "# 4. Train a New Model on the Enhanced Data\n",
    "###############################################################################\n",
    "# The feature_engineer.df now has new features\n",
    "df_enhanced = feature_engineer.df\n",
    "X_enhanced = df_enhanced.drop(columns=[\"Level\"])\n",
    "y_enhanced = df_enhanced[\"Level\"]\n",
    "\n",
    "X_train_enhanced, X_test_enhanced, y_train_enhanced, y_test_enhanced = train_test_split(\n",
    "    X_enhanced, y_enhanced, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "enhanced_model = RandomForestClassifier(\n",
    "    n_estimators=50, max_depth=5, min_samples_split=10, random_state=42\n",
    ")\n",
    "enhanced_model.fit(X_train_enhanced, y_train_enhanced)\n",
    "\n",
    "# Evaluate the enhanced model\n",
    "y_pred_enhanced = enhanced_model.predict(X_test_enhanced)\n",
    "enhanced_model_results = {\n",
    "    \"Accuracy\": accuracy_score(y_test_enhanced, y_pred_enhanced),\n",
    "    \"Precision\": precision_score(y_test_enhanced, y_pred_enhanced, average='weighted'),\n",
    "    \"Recall\": recall_score(y_test_enhanced, y_pred_enhanced, average='weighted'),\n",
    "    \"F1 Score\": f1_score(y_test_enhanced, y_pred_enhanced, average='weighted')\n",
    "}\n",
    "\n",
    "###############################################################################\n",
    "# 5. Compare Baseline vs. Enhanced\n",
    "###############################################################################\n",
    "print(\"\\nComparison Between Baseline and Enhanced Model:\")\n",
    "print(\"Baseline Model:\", baseline_results)\n",
    "print(\"Enhanced Model:\", enhanced_model_results)\n",
    "\n",
    "# Statistical Test\n",
    "baseline_scores = np.array(list(baseline_results.values()))\n",
    "enhanced_scores = np.array(list(enhanced_model_results.values()))\n",
    "t_stat_cancer, p_value_cancer = ttest_rel(baseline_scores, enhanced_scores)\n",
    "print(f\"\\nPaired T-Test: t={t_stat_cancer:.4f}, p={p_value_cancer:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 2: Amsterdam Rental Prices (Regression)\n",
    "Goal: Predict rental prices of properties in Amsterdam.\n",
    "\n",
    "- Type: Regression (Target: realSum)\n",
    "- Baseline Model: RandomForestRegressor\n",
    "- Enhanced Model: Random Forest with Engineered Features\n",
    "- Comparison Metrics: R², RMSE, MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amit\\AppData\\Local\\Temp\\ipykernel_3876\\2741767089.py:10: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_amsterdam[col] = df_amsterdam[col].replace({\n",
      "C:\\Users\\Amit\\AppData\\Local\\Temp\\ipykernel_3876\\2741767089.py:10: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_amsterdam[col] = df_amsterdam[col].replace({\n",
      "C:\\Users\\Amit\\AppData\\Local\\Temp\\ipykernel_3876\\2741767089.py:10: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_amsterdam[col] = df_amsterdam[col].replace({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Amsterdam Baseline Model ===\n",
      "{'R²': 0.481964703404132, 'RMSE': 224.68364147185966, 'MAE': 148.43478257371325}\n",
      "[FeatureEngineering] Starting pipeline...\n",
      "[FeatureEngineering] Generating new features...\n",
      "[FeatureEngineering] Added 840 new features.\n",
      "Sample new features: ['bedrooms_div_rest_index', 'multi_div_room_type_Entire home/apt', 'host_is_superhost_plus_room_type_Shared room', 'room_shared_times_rest_index', 'room_shared_times_room_type_Private room', 'cleanliness_rating_div_bedrooms', 'rest_index_minus_rest_index_norm', 'dist_div_rest_index', 'person_capacity_plus_rest_index', 'room_shared_times_room_type_Shared room']\n",
      "[FeatureEngineering] Filtering features based on correlation & variance...\n",
      "[FeatureEngineering] Dropping 198\n",
      "[FeatureEngineering] Dropping 70\n",
      "[FeatureEngineering] 579 new features were retained after filtering.\n",
      "[FeatureEngineering] Checking final set of features for importance...\n",
      "[FeatureEngineering] Computing feature importance via SHAP & feature_importances_...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|===================| 1083/1103 [00:44<00:00]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[FeatureEngineering] Important Features (Combined SHAP + Permutation):\n",
      " person_capacity_plus_room_type_Entire home/apt     17.100581\n",
      "person_capacity_minus_metro_dist                   14.274584\n",
      "room_private_div_person_capacity                    9.024589\n",
      "person_capacity_times_room_type_Entire home/apt     7.837398\n",
      "bedrooms_plus_room_type_Entire home/apt             7.156334\n",
      "person_capacity_minus_dist                          7.011777\n",
      "guest_satisfaction_overall_plus_attr_index_norm     6.579408\n",
      "person_capacity_div_room_type_Entire home/apt       5.831489\n",
      "bedrooms_minus_dist                                 4.501200\n",
      "attr_index_norm_div_room_type_Entire home/apt       4.000648\n",
      "bedrooms_times_attr_index_norm                      3.653305\n",
      "person_capacity_times_attr_index                    3.164379\n",
      "guest_satisfaction_overall_plus_rest_index_norm     2.550856\n",
      "attr_index_norm_times_room_type_Entire home/apt     2.537424\n",
      "person_capacity_div_dist                            2.532622\n",
      "person_capacity_minus_room_type_Private room        2.411112\n",
      "multi_times_metro_dist                              2.397141\n",
      "attr_index_times_room_type_Entire home/apt          2.224469\n",
      "bedrooms_times_attr_index                           2.162893\n",
      "person_capacity_times_rest_index                    2.139179\n",
      "dtype: float64\n",
      "[FeatureEngineering] Training & evaluating final model with new features...\n",
      "\n",
      "[FeatureEngineering] Model Performance with Engineered Features: {'R²': 0.5802285746091124, 'RMSE': 202.25450979217024}\n",
      "\n",
      "[FeatureEngineering] Summary:\n",
      "- 840 new features generated\n",
      "- 594 total features after filtering\n",
      "- 579 new features retained\n",
      "\n",
      "=== Amsterdam Enhanced Model ===\n",
      "{'R²': 0.5894104020939616, 'RMSE': 200.0302829926119, 'MAE': 134.64565749865147}\n",
      "\n",
      "Comparison Baseline vs. Enhanced (Amsterdam)\n",
      "Baseline: {'R²': 0.481964703404132, 'RMSE': 224.68364147185966, 'MAE': 148.43478257371325}\n",
      "Enhanced: {'R²': 0.5894104020939616, 'RMSE': 200.0302829926119, 'MAE': 134.64565749865147}\n",
      "\n",
      "Paired T-Test: t=1.7833, p=0.2165\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# 1. LOAD & CLEAN (Baseline)\n",
    "###############################################################################\n",
    "dataset_path = os.path.abspath(\"../Data/amsterdam_weekdays.csv\")\n",
    "df_amsterdam = pd.read_csv(dataset_path)\n",
    "\n",
    "# Convert booleans/strings to 0/1\n",
    "for col in [\"host_is_superhost\", \"room_private\", \"room_shared\"]:\n",
    "    if col in df_amsterdam.columns:\n",
    "        df_amsterdam[col] = df_amsterdam[col].replace({\n",
    "            False: 0, True: 1, \"FALSE\": 0, \"TRUE\": 1\n",
    "        }).astype(int)\n",
    "\n",
    "# One-hot encode 'room_type' if present\n",
    "if \"room_type\" in df_amsterdam.columns:\n",
    "    df_amsterdam = pd.get_dummies(df_amsterdam, columns=[\"room_type\"], prefix=\"room_type\")\n",
    "\n",
    "# Convert any leftover bools\n",
    "bool_cols = df_amsterdam.select_dtypes(include='bool').columns\n",
    "df_amsterdam[bool_cols] = df_amsterdam[bool_cols].astype(int)\n",
    "\n",
    "target_col = \"realSum\"\n",
    "if target_col not in df_amsterdam.columns:\n",
    "    raise KeyError(f\"Target '{target_col}' not found in amsterdam_weekdays.csv\")\n",
    "\n",
    "# Save cleaned\n",
    "cleaned_path = os.path.abspath(\"../Data/amsterdam_weekdays_clean.csv\")\n",
    "df_amsterdam.to_csv(cleaned_path, index=False)\n",
    "\n",
    "# Prepare baseline data\n",
    "X_amst = df_amsterdam.drop(columns=[target_col])\n",
    "y_amst = df_amsterdam[target_col]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_amst_scaled = pd.DataFrame(scaler.fit_transform(X_amst), columns=X_amst.columns)\n",
    "\n",
    "X_train_amst, X_test_amst, y_train_amst, y_test_amst = train_test_split(\n",
    "    X_amst_scaled, y_amst, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Baseline model\n",
    "baseline_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "baseline_model.fit(X_train_amst, y_train_amst)\n",
    "\n",
    "# Evaluate baseline\n",
    "y_pred_baseline = baseline_model.predict(X_test_amst)\n",
    "baseline_metrics = {\n",
    "    \"R²\": r2_score(y_test_amst, y_pred_baseline),\n",
    "    \"RMSE\": np.sqrt(mean_squared_error(y_test_amst, y_pred_baseline)),\n",
    "    \"MAE\": mean_absolute_error(y_test_amst, y_pred_baseline),\n",
    "}\n",
    "print(\"\\n=== Amsterdam Baseline Model ===\")\n",
    "print(baseline_metrics)\n",
    "\n",
    "###############################################################################\n",
    "# 2. SEMI-AUTOMATED FEATURE ENGINEERING\n",
    "###############################################################################\n",
    "\n",
    "feature_engineer = SemiAutomatedFeatureEngineering(\n",
    "    df_amsterdam.copy(), \n",
    "    target_column=target_col,\n",
    "    task=\"regression\",\n",
    "    correlation_threshold=0.05,\n",
    "    variance_threshold=0.01\n",
    ")\n",
    "pipeline_results = feature_engineer.run_pipeline()\n",
    "\n",
    "# The pipeline's final DataFrame with new features\n",
    "df_amst_enh = feature_engineer.df\n",
    "X_enh = df_amst_enh.drop(columns=[target_col])\n",
    "y_enh = df_amst_enh[target_col]\n",
    "\n",
    "# Scale again with new features\n",
    "X_enh_scaled = pd.DataFrame(scaler.fit_transform(X_enh), columns=X_enh.columns)\n",
    "\n",
    "X_train_enh, X_test_enh, y_train_enh, y_test_enh = train_test_split(\n",
    "    X_enh_scaled, y_enh, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "enh_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "enh_model.fit(X_train_enh, y_train_enh)\n",
    "\n",
    "y_pred_enh = enh_model.predict(X_test_enh)\n",
    "enhanced_metrics = {\n",
    "    \"R²\": r2_score(y_test_enh, y_pred_enh),\n",
    "    \"RMSE\": np.sqrt(mean_squared_error(y_test_enh, y_pred_enh)),\n",
    "    \"MAE\": mean_absolute_error(y_test_enh, y_pred_enh),\n",
    "}\n",
    "print(\"\\n=== Amsterdam Enhanced Model ===\")\n",
    "print(enhanced_metrics)\n",
    "\n",
    "# Compare\n",
    "print(\"\\nComparison Baseline vs. Enhanced (Amsterdam)\")\n",
    "print(\"Baseline:\", baseline_metrics)\n",
    "print(\"Enhanced:\", enhanced_metrics)\n",
    "\n",
    "# Statistical Test (Paired T-Test)\n",
    "baseline_scores = np.array(list(baseline_metrics.values()))\n",
    "enhanced_scores = np.array(list(enhanced_metrics.values()))\n",
    "t_stat_ams, p_value_ams = ttest_rel(baseline_scores, enhanced_scores) \n",
    "print(f\"\\nPaired T-Test: t={t_stat_ams:.4f}, p={p_value_ams:.4f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 3: Student Performance (Classification)\n",
    "Goal: Predict whether a student passes based on academic and lifestyle factors.\n",
    "\n",
    "- Type: Classification (Target: Passed)\n",
    "- Baseline Model: RandomForestClassifier\n",
    "- Enhanced Model: Random Forest with Engineered Features\n",
    "- Comparison Metrics: Accuracy, Precision, Recall, F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amit\\AppData\\Local\\Temp\\ipykernel_3876\\3973346227.py:22: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  .replace({\"true\": 1, \"false\": 0, \"yes\": 1, \"no\": 0})\n",
      "C:\\Users\\Amit\\AppData\\Local\\Temp\\ipykernel_3876\\3973346227.py:22: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  .replace({\"true\": 1, \"false\": 0, \"yes\": 1, \"no\": 0})\n",
      "C:\\Users\\Amit\\AppData\\Local\\Temp\\ipykernel_3876\\3973346227.py:22: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  .replace({\"true\": 1, \"false\": 0, \"yes\": 1, \"no\": 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Students Baseline Model ===\n",
      "{'Accuracy': 0.9916792738275341, 'Precision': 0.9834277821391052, 'Recall': 0.9916792738275341, 'F1 Score': 0.9875362916732983}\n",
      "[FeatureEngineering] Starting pipeline...\n",
      "[FeatureEngineering] Generating new features...\n",
      "[FeatureEngineering] Added 1512 new features.\n",
      "Sample new features: ['Teacher_Quality_Medium_plus_Peer_Influence_Neutral', 'Extracurricular_Activities_plus_Learning_Disabilities', 'Exam_Score_plus_Teacher_Quality_Medium', 'Sleep_Hours_plus_Parental_Education_Level_High School', 'Parental_Involvement_Medium_div_Teacher_Quality_Medium', 'Access_to_Resources_Low_plus_Teacher_Quality_Low', 'Attendance_div_Family_Income_Low', 'Learning_Disabilities_div_Exam_Score', 'Motivation_Level_Low_plus_Gender_Male', 'Parental_Education_Level_High School_minus_Distance_from_Home_Moderate']\n",
      "[FeatureEngineering] Filtering features based on correlation & variance...\n",
      "[FeatureEngineering] Dropping 1217\n",
      "[FeatureEngineering] Dropping 21\n",
      "[FeatureEngineering] 296 new features were retained after filtering.\n",
      "[FeatureEngineering] Checking final set of features for importance...\n",
      "[FeatureEngineering] Computing feature importance via SHAP & feature_importances_...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|=================== | 12434/13214 [00:14<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[FeatureEngineering] Important Features (Combined SHAP + Permutation):\n",
      " Attendance       0.00005\n",
      "Hours_Studied    0.00005\n",
      "dtype: float64\n",
      "[FeatureEngineering] Training & evaluating final model with new features...\n",
      "\n",
      "[FeatureEngineering] Model Performance with Engineered Features: {'Accuracy': 1.0, 'Precision': 1.0, 'Recall': 1.0, 'F1 Score': 1.0}\n",
      "\n",
      "[FeatureEngineering] Summary:\n",
      "- 1512 new features generated\n",
      "- 303 total features after filtering\n",
      "- 296 new features retained\n",
      "\n",
      "=== Students Enhanced Model ===\n",
      "{'Accuracy': 1.0, 'Precision': 1.0, 'Recall': 1.0, 'F1 Score': 1.0}\n",
      "\n",
      "Compare Baseline vs. Enhanced (Students):\n",
      "Baseline: {'Accuracy': 0.9916792738275341, 'Precision': 0.9834277821391052, 'Recall': 0.9916792738275341, 'F1 Score': 0.9875362916732983}\n",
      "Enhanced: {'Accuracy': 1.0, 'Precision': 1.0, 'Recall': 1.0, 'F1 Score': 1.0}\n",
      "\n",
      "Paired T-Test: t=-5.7796, p=0.0103\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# 1. BASELINE\n",
    "###############################################################################\n",
    "df_students = pd.read_csv(\"../Data/StudentPerformanceFactors.csv\")\n",
    "\n",
    "# Create \"Passed\" from \"Exam_Score\"\n",
    "if \"Exam_Score\" not in df_students.columns:\n",
    "    raise KeyError(\"No 'Exam_Score' column found; cannot create 'Passed' column.\")\n",
    "df_students[\"Passed\"] = (df_students[\"Exam_Score\"] >= 60).astype(int)\n",
    "\n",
    "# Convert boolean-like columns\n",
    "bool_cols_list = [\"Internet_Access\", \"Learning_Disabilities\", \"Extracurricular_Activities\"]\n",
    "for col in bool_cols_list:\n",
    "    if col in df_students.columns:\n",
    "        if df_students[col].dtype == bool:\n",
    "            df_students[col] = df_students[col].astype(int)\n",
    "        else:\n",
    "            df_students[col] = (\n",
    "                df_students[col]\n",
    "                .astype(str)\n",
    "                .str.lower()\n",
    "                .replace({\"true\": 1, \"false\": 0, \"yes\": 1, \"no\": 0})\n",
    "                .fillna(0)\n",
    "                .astype(int)\n",
    "            )\n",
    "\n",
    "# Identify other categorical columns & one-hot\n",
    "categorical_cols = [\n",
    "    \"Parental_Involvement\", \"Access_to_Resources\", \"Motivation_Level\",\n",
    "    \"Family_Income\", \"Teacher_Quality\", \"School_Type\", \"Peer_Influence\",\n",
    "    \"Parental_Education_Level\", \"Distance_from_Home\", \"Gender\"\n",
    "]\n",
    "existing_cat = [c for c in categorical_cols if c in df_students.columns]\n",
    "if existing_cat:\n",
    "    df_students = pd.get_dummies(df_students, columns=existing_cat, drop_first=True)\n",
    "\n",
    "# Convert leftover bool\n",
    "bool_cols2 = df_students.select_dtypes(include='bool').columns\n",
    "df_students[bool_cols2] = df_students[bool_cols2].astype(int)\n",
    "\n",
    "target_col = \"Passed\"\n",
    "if target_col not in df_students.columns:\n",
    "    raise KeyError(f\"Target '{target_col}' not found in student dataset.\")\n",
    "\n",
    "# For baseline, we'll remove \"Exam_Score\" from features to avoid direct leak\n",
    "X_stud = df_students.drop(columns=[\"Exam_Score\", target_col])\n",
    "y_stud = df_students[target_col]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_stud_scaled = pd.DataFrame(scaler.fit_transform(X_stud), columns=X_stud.columns)\n",
    "\n",
    "X_train_stud, X_test_stud, y_train_stud, y_test_stud = train_test_split(\n",
    "    X_stud_scaled, y_stud, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "baseline_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "baseline_model.fit(X_train_stud, y_train_stud)\n",
    "\n",
    "y_pred_base = baseline_model.predict(X_test_stud)\n",
    "baseline_metrics_stu = {\n",
    "    \"Accuracy\": accuracy_score(y_test_stud, y_pred_base),\n",
    "    \"Precision\": precision_score(y_test_stud, y_pred_base, average='weighted', zero_division=0),\n",
    "    \"Recall\": recall_score(y_test_stud, y_pred_base, average='weighted', zero_division=0),\n",
    "    \"F1 Score\": f1_score(y_test_stud, y_pred_base, average='weighted', zero_division=0),\n",
    "}\n",
    "print(\"\\n=== Students Baseline Model ===\")\n",
    "print(baseline_metrics_stu)\n",
    "\n",
    "###############################################################################\n",
    "# 2. SEMI-AUTOMATED FEATURE ENGINEERING\n",
    "###############################################################################\n",
    "\n",
    "feature_engineer = SemiAutomatedFeatureEngineering(\n",
    "    df_students.copy(),\n",
    "    target_column=target_col,\n",
    "    task=\"classification\"\n",
    ")\n",
    "pipeline_results = feature_engineer.run_pipeline()\n",
    "\n",
    "df_stud_enh = feature_engineer.df\n",
    "X_stud_enh = df_stud_enh.drop(columns=[\"Exam_Score\", target_col], errors='ignore')\n",
    "y_stud_enh = df_stud_enh[target_col]\n",
    "\n",
    "X_stud_enh_scaled = pd.DataFrame(scaler.fit_transform(X_stud_enh), columns=X_stud_enh.columns)\n",
    "\n",
    "X_train_stud2, X_test_stud2, y_train_stud2, y_test_stud2 = train_test_split(\n",
    "    X_stud_enh_scaled, y_stud_enh, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "enh_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "enh_model.fit(X_train_stud2, y_train_stud2)\n",
    "\n",
    "y_pred_enh = enh_model.predict(X_test_stud2)\n",
    "enhanced_metrics_stu = {\n",
    "    \"Accuracy\": accuracy_score(y_test_stud2, y_pred_enh),\n",
    "    \"Precision\": precision_score(y_test_stud2, y_pred_enh, average='weighted', zero_division=0),\n",
    "    \"Recall\": recall_score(y_test_stud2, y_pred_enh, average='weighted', zero_division=0),\n",
    "    \"F1 Score\": f1_score(y_test_stud2, y_pred_enh, average='weighted', zero_division=0),\n",
    "}\n",
    "print(\"\\n=== Students Enhanced Model ===\")\n",
    "print(enhanced_metrics_stu)\n",
    "\n",
    "# Compare\n",
    "print(\"\\nCompare Baseline vs. Enhanced (Students):\")\n",
    "print(\"Baseline:\", baseline_metrics_stu)\n",
    "print(\"Enhanced:\", enhanced_metrics_stu)\n",
    "\n",
    "# Statistical Test (Paired T-Test)\n",
    "baseline_scores_stu = np.array(list(baseline_metrics_stu.values()))\n",
    "enhanced_scores_stu = np.array(list(enhanced_metrics_stu.values()))\n",
    "t_stat_stu, p_value_stu = ttest_rel(baseline_scores_stu, enhanced_scores_stu) \n",
    "print(f\"\\nPaired T-Test: t={t_stat_stu:.4f}, p={p_value_stu:.4f}\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 4: Life expectancy (Regression)\n",
    "Goal: Predict life expectancy \n",
    "- Type: Regression (Target: Life expectancy)\n",
    "- Baseline Model: RandomForestRegressor\n",
    "- Enhanced Model: Random Forest with Engineered Features\n",
    "- Comparison Metrics: R², RMSE, MAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Life Expectancy Dataset...\n",
      "Columns in the dataset: ['Country', 'Year', 'Status', 'Life expectancy ', 'Adult Mortality', 'infant deaths', 'Alcohol', 'percentage expenditure', 'Hepatitis B', 'Measles ', ' BMI ', 'under-five deaths ', 'Polio', 'Total expenditure', 'Diphtheria ', ' HIV/AIDS', 'GDP', 'Population', ' thinness  1-19 years', ' thinness 5-9 years', 'Income composition of resources', 'Schooling']\n",
      "Low-cardinality columns: ['Status']\n",
      "High-cardinality columns: ['Country']\n",
      "\n",
      "=== Life Expectancy Baseline Model ===\n",
      "{'R²': 0.965400247335062, 'RMSE': 1.731666654894535, 'MAE': 1.1672954948580996}\n",
      "\n",
      "Running Feature Engineering on Life Expectancy Data...\n",
      "[FeatureEngineering] Starting pipeline...\n",
      "[FeatureEngineering] Generating new features...\n",
      "[FeatureEngineering] Added 840 new features.\n",
      "Sample new features: ['infant deaths_plus_Total expenditure', 'Measles _plus_ BMI ', 'Hepatitis B_times_GDP', 'Polio_div_GDP', 'Year_minus_ thinness 5-9 years', 'percentage expenditure_minus_Country_freq', 'Total expenditure_minus_GDP', 'Alcohol_plus_Income composition of resources', 'Status_Developing_times_Country_freq', 'percentage expenditure_plus_Hepatitis B']\n",
      "[FeatureEngineering] Filtering features based on correlation & variance...\n",
      "[FeatureEngineering] Dropping 141\n",
      "[FeatureEngineering] Dropping 21\n",
      "[FeatureEngineering] 681 new features were retained after filtering.\n",
      "[FeatureEngineering] Checking final set of features for importance...\n",
      "[FeatureEngineering] Computing feature importance via SHAP & feature_importances_...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|===================| 2937/2938 [04:46<00:00]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[FeatureEngineering] Important Features (Combined SHAP + Permutation):\n",
      " under-five deaths _times_ HIV/AIDS                         1.228899\n",
      "Adult Mortality_div_Schooling                              0.517097\n",
      "Income composition of resources_minus_Status_Developing    0.360611\n",
      " HIV/AIDS_minus_Income composition of resources            0.290027\n",
      " HIV/AIDS_minus_Schooling                                  0.256694\n",
      "Year_div_Income composition of resources                   0.170533\n",
      "infant deaths_times_ HIV/AIDS                              0.166747\n",
      "Diphtheria _times_Income composition of resources          0.159529\n",
      "Year_times_Income composition of resources                 0.134318\n",
      "Diphtheria _div_ HIV/AIDS                                  0.104327\n",
      " BMI _div_ HIV/AIDS                                        0.086058\n",
      "Year_minus_Adult Mortality                                 0.083090\n",
      "Adult Mortality_div_Income composition of resources        0.073742\n",
      "infant deaths_div_under-five deaths                        0.066974\n",
      "Adult Mortality_minus_Alcohol                              0.066931\n",
      "Adult Mortality_minus_ BMI                                 0.061781\n",
      " thinness 5-9 years_div_Income composition of resources    0.047676\n",
      "Adult Mortality_plus_ thinness 5-9 years                   0.045715\n",
      "Adult Mortality_minus_Diphtheria                           0.041563\n",
      "Adult Mortality_plus_ thinness  1-19 years                 0.035760\n",
      "dtype: float64\n",
      "[FeatureEngineering] Training & evaluating final model with new features...\n",
      "\n",
      "[FeatureEngineering] Model Performance with Engineered Features: {'R²': 0.9705314507390108, 'RMSE': 1.5981118621077552}\n",
      "\n",
      "[FeatureEngineering] Summary:\n",
      "- 840 new features generated\n",
      "- 700 total features after filtering\n",
      "- 681 new features retained\n",
      "\n",
      "Comparison Between Baseline and Enhanced Model (Life Expectancy Prediction):\n",
      "Baseline Model: {'R²': 0.965400247335062, 'RMSE': 1.731666654894535, 'MAE': 1.1672954948580996}\n",
      "Enhanced Model: {'R²': 0.967667495248192, 'RMSE': 1.6739692352458213, 'MAE': 1.0854178045882663}\n",
      "\n",
      "Paired T-Test: t=1.8299, p=0.2088\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "###############################################################################\n",
    "# 1. Load & Preprocess the Data\n",
    "###############################################################################\n",
    "print(\"\\nLoading Life Expectancy Dataset...\")\n",
    "dataset_path = \"../Data/LifeExpect.csv\"\n",
    "df_life = pd.read_csv(dataset_path)\n",
    "\n",
    "print(\"Columns in the dataset:\", df_life.columns.tolist())\n",
    "\n",
    "# Drop unnecessary columns if applicable\n",
    "drop_columns = [\"ID\", \"Index\"]\n",
    "df_life.drop(columns=[col for col in drop_columns if col in df_life.columns], inplace=True)\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_cols = df_life.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "categorical_cols = df_life.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "# Handle missing values\n",
    "df_life[numeric_cols] = df_life[numeric_cols].fillna(df_life[numeric_cols].median())\n",
    "for col in categorical_cols:\n",
    "    df_life[col] = df_life[col].fillna(df_life[col].mode()[0])\n",
    "\n",
    "# Convert boolean columns to numeric (0/1)\n",
    "bool_cols = df_life.select_dtypes(include='bool').columns\n",
    "df_life[bool_cols] = df_life[bool_cols].astype(int)\n",
    "\n",
    "# Identify categorical columns\n",
    "object_cols = categorical_cols.tolist()\n",
    "CARDINALITY_THRESHOLD = 50\n",
    "low_cardinality_cols = [col for col in object_cols if df_life[col].nunique() <= CARDINALITY_THRESHOLD]\n",
    "high_cardinality_cols = [col for col in object_cols if df_life[col].nunique() > CARDINALITY_THRESHOLD]\n",
    "\n",
    "print(f\"Low-cardinality columns: {low_cardinality_cols}\")\n",
    "print(f\"High-cardinality columns: {high_cardinality_cols}\")\n",
    "\n",
    "# One-Hot Encoding for low-cardinality categorical features\n",
    "df_life = pd.get_dummies(df_life, columns=low_cardinality_cols, drop_first=True)\n",
    "\n",
    "# Frequency Encoding for high-cardinality categorical features\n",
    "for col in high_cardinality_cols:\n",
    "    freq_map = df_life[col].value_counts(normalize=True)\n",
    "    df_life[f\"{col}_freq\"] = df_life[col].map(freq_map)\n",
    "df_life.drop(columns=high_cardinality_cols, inplace=True, errors='ignore')\n",
    "\n",
    "# Ensure all boolean columns are numeric (0/1)\n",
    "bool_cols_after = df_life.select_dtypes(include='bool').columns\n",
    "df_life[bool_cols_after] = df_life[bool_cols_after].astype(int)\n",
    "\n",
    "# Ensure all columns are numeric\n",
    "df_life = df_life.select_dtypes(include=[\"int\", \"float\"])\n",
    "\n",
    "# Define the target variable (Life Expectancy)\n",
    "target_col = \"Life expectancy \" \n",
    "if target_col not in df_life.columns:\n",
    "    raise KeyError(f\"Target column '{target_col}' not found. Available: {df_life.columns.tolist()}\")\n",
    "\n",
    "# Split the data into features and target\n",
    "X_life = df_life.drop(columns=[target_col])\n",
    "y_life = df_life[target_col]\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_life_scaled = pd.DataFrame(scaler.fit_transform(X_life), columns=X_life.columns)\n",
    "\n",
    "# Save cleaned dataset\n",
    "cleaned_path = \"../Data/life_expectancy_clean.csv\"\n",
    "df_life.to_csv(cleaned_path, index=False)\n",
    "\n",
    "###############################################################################\n",
    "# 2. Baseline Model Training (XGBRegressor)\n",
    "###############################################################################\n",
    "X_train_life, X_test_life, y_train_life, y_test_life = train_test_split(\n",
    "    X_life_scaled, y_life, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train a baseline model with XGBoost\n",
    "baseline_model = XGBRegressor(n_estimators=100, max_depth=7, random_state=42, n_jobs=-1)\n",
    "baseline_model.fit(X_train_life, y_train_life)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_life = baseline_model.predict(X_test_life)\n",
    "\n",
    "# Calculate regression metrics\n",
    "baseline_results_exp = {\n",
    "    \"R²\": r2_score(y_test_life, y_pred_life),\n",
    "    \"RMSE\": np.sqrt(mean_squared_error(y_test_life, y_pred_life)),\n",
    "    \"MAE\": mean_absolute_error(y_test_life, y_pred_life),\n",
    "}\n",
    "\n",
    "print(\"\\n=== Life Expectancy Baseline Model ===\")\n",
    "print(baseline_results_exp)\n",
    "\n",
    "###############################################################################\n",
    "# 3. Feature Engineering (SemiAutomatedFeatureEngineering)\n",
    "###############################################################################\n",
    "print(\"\\nRunning Feature Engineering on Life Expectancy Data...\")\n",
    "\n",
    "feature_engineer = SemiAutomatedFeatureEngineering(\n",
    "    df_life.copy(),\n",
    "    target_column=target_col,\n",
    "    task=\"regression\",\n",
    "    correlation_threshold=0.1,  \n",
    "    variance_threshold=0.05\n",
    ")\n",
    "\n",
    "feature_engineer.run_pipeline()\n",
    "\n",
    "###############################################################################\n",
    "# 4. Train a New Model on the Enhanced Data\n",
    "###############################################################################\n",
    "# Get the enhanced dataset\n",
    "df_life_enhanced = feature_engineer.df\n",
    "X_life_enhanced = df_life_enhanced.drop(columns=[target_col])\n",
    "y_life_enhanced = df_life_enhanced[target_col]\n",
    "\n",
    "# Scale enhanced features\n",
    "X_life_enhanced_scaled = pd.DataFrame(scaler.fit_transform(X_life_enhanced), columns=X_life_enhanced.columns)\n",
    "\n",
    "X_train_life_enh, X_test_life_enh, y_train_life_enh, y_test_life_enh = train_test_split(\n",
    "    X_life_enhanced_scaled, y_life_enhanced, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train an enhanced model with XGBoost\n",
    "enhanced_model = XGBRegressor(n_estimators=100, max_depth=7, random_state=42, n_jobs=-1)\n",
    "enhanced_model.fit(X_train_life_enh, y_train_life_enh)\n",
    "\n",
    "# Evaluate the enhanced model\n",
    "y_pred_life_enh = enhanced_model.predict(X_test_life_enh)\n",
    "enhanced_results_exp = {\n",
    "    \"R²\": r2_score(y_test_life_enh, y_pred_life_enh),\n",
    "    \"RMSE\": np.sqrt(mean_squared_error(y_test_life_enh, y_pred_life_enh)),\n",
    "    \"MAE\": mean_absolute_error(y_test_life_enh, y_pred_life_enh),\n",
    "}\n",
    "\n",
    "###############################################################################\n",
    "# 5. Compare Baseline vs. Enhanced\n",
    "###############################################################################\n",
    "print(\"\\nComparison Between Baseline and Enhanced Model (Life Expectancy Prediction):\")\n",
    "print(\"Baseline Model:\", baseline_results_exp)\n",
    "print(\"Enhanced Model:\", enhanced_results_exp)\n",
    "\n",
    "# Statistical Test (Paired T-Test)\n",
    "baseline_scores = np.array(list(baseline_results_exp.values()))\n",
    "enhanced_scores = np.array(list(enhanced_results_exp.values()))\n",
    "t_stat, p_value = ttest_rel(baseline_scores, enhanced_scores) \n",
    "print(f\"\\nPaired T-Test: t={t_stat:.4f}, p={p_value:.4f}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comparison to the autofeat library's algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 12:05:57,074 INFO: [AutoFeat] The 1 step feature engineering process could generate up to 147 features.\n",
      "2025-03-10 12:05:57,076 INFO: [AutoFeat] With 882 data points this new feature matrix would use about 0.00 gb of space.\n",
      "2025-03-10 12:05:57,078 INFO: [feateng] Step 1: transformation of original features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying AutoFeat on Amsterdam Dataset...\n",
      "[feateng]               0/             21 features transformed\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 12:05:58,369 INFO: [feateng] Generated 65 transformed features from 21 original features - done.\n",
      "2025-03-10 12:05:58,372 INFO: [feateng] Generated altogether 65 new features in 1 steps\n",
      "2025-03-10 12:05:58,373 INFO: [feateng] Removing correlated features, as well as additions at the highest level\n",
      "2025-03-10 12:05:58,380 INFO: [feateng] Generated a total of 65 additional features\n",
      "2025-03-10 12:05:58,385 INFO: [featsel] Feature selection run 1/5\n",
      "2025-03-10 12:05:58,536 INFO: [featsel] Feature selection run 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[featsel] Scaling data...done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 12:05:58,678 INFO: [featsel] Feature selection run 3/5\n",
      "2025-03-10 12:05:58,828 INFO: [featsel] Feature selection run 4/5\n",
      "2025-03-10 12:05:58,966 INFO: [featsel] Feature selection run 5/5\n",
      "2025-03-10 12:05:59,108 INFO: [featsel] 15 features after 5 feature selection runs\n",
      "c:\\Python312\\Lib\\site-packages\\autofeat\\featsel.py:270: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  if np.max(np.abs(correlations[c].ravel()[:i])) < 0.9:\n",
      "2025-03-10 12:05:59,111 INFO: [featsel] 12 features after correlation filtering\n",
      "2025-03-10 12:05:59,142 INFO: [featsel] 11 features after noise filtering\n",
      "2025-03-10 12:05:59,144 INFO: [AutoFeat] Computing 6 new features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AutoFeat]     5/    6 new features\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 12:05:59,671 INFO: [AutoFeat]     6/    6 new features ...done.\n",
      "2025-03-10 12:05:59,673 INFO: [AutoFeat] Final dataframe with 27 feature columns (6 new).\n",
      "2025-03-10 12:05:59,674 INFO: [AutoFeat] Training final regression model.\n",
      "2025-03-10 12:05:59,690 INFO: [AutoFeat] Trained model: largest coefficients:\n",
      "2025-03-10 12:05:59,692 INFO: 402.11845964909133\n",
      "2025-03-10 12:05:59,693 INFO: 92.500695 * room_type_Entire home/apt\n",
      "2025-03-10 12:05:59,693 INFO: 79.735985 * person_capacity\n",
      "2025-03-10 12:05:59,694 INFO: 76.659469 * attr_index\n",
      "2025-03-10 12:05:59,696 INFO: 62.997990 * exp(guest_satisfaction_overall)\n",
      "2025-03-10 12:05:59,697 INFO: -41.016056 * Abs(lng)\n",
      "2025-03-10 12:05:59,698 INFO: 36.235626 * bedrooms\n",
      "2025-03-10 12:05:59,699 INFO: 28.353970 * Unnamed0**2\n",
      "2025-03-10 12:05:59,700 INFO: 24.991998 * bedrooms**2\n",
      "2025-03-10 12:05:59,701 INFO: 19.892433 * exp(person_capacity)\n",
      "2025-03-10 12:05:59,702 INFO: -10.822153 * dist\n",
      "2025-03-10 12:05:59,702 INFO: 4.142612 * bedrooms**3\n",
      "2025-03-10 12:05:59,705 INFO: [AutoFeat] Final score: 0.4533\n",
      "2025-03-10 12:05:59,707 INFO: [AutoFeat] Computing 6 new features.\n",
      "2025-03-10 12:05:59,718 INFO: [AutoFeat]     6/    6 new features ...done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoFeat generated 6 new features.s\n",
      "\n",
      "=== AutoFeat Model Performance ===\n",
      "{'R²': 0.5516051871390136, 'RMSE': 209.03647251328667, 'MAE': 144.84566065118793}\n",
      "\n",
      "Loading Results from Semi-Automated Feature Engineering...\n",
      "\n",
      "=== Semi-Automated Feature Engineering Performance ===\n",
      "{'R²': 0.5894, 'RMSE': 200.03, 'MAE': 134.65}\n",
      "\n",
      "Comparison Between Semi-Automated and AutoFeat (Amsterdam)\n",
      "Semi-Automated: {'R²': 0.5894, 'RMSE': 200.03, 'MAE': 134.65}\n",
      "AutoFeat: {'R²': 0.5516051871390136, 'RMSE': 209.03647251328667, 'MAE': 144.84566065118793}\n",
      "\n",
      "Paired T-Test (Semi-Automated vs. AutoFeat): t=-1.9770, p-value=0.1867\n",
      "No significant difference found between Semi-Automated and AutoFeat approach.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from autofeat import AutoFeatRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import ttest_rel  # Using paired t-test\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "###############################################################################\n",
    "# 1. Load & Preprocess the Data\n",
    "###############################################################################\n",
    "dataset_path = os.path.abspath(\"../Data/amsterdam_weekdays_clean.csv\")\n",
    "df_amsterdam = pd.read_csv(dataset_path)\n",
    "\n",
    "# Define Features and Target\n",
    "target_col = \"realSum\"\n",
    "X = df_amsterdam.drop(columns=[target_col])  # Features\n",
    "y = df_amsterdam[target_col]  # Target (Regression Task)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "###############################################################################\n",
    "# 2. Apply AutoFeat for Feature Engineering\n",
    "###############################################################################\n",
    "print(\"\\nApplying AutoFeat on Amsterdam Dataset...\")\n",
    "\n",
    "# Initialize AutoFeat Regressor\n",
    "auto_feat = AutoFeatRegressor(verbose=1, feateng_steps=1)\n",
    "\n",
    "# Fit and Transform Training Data\n",
    "X_train_autofeat = auto_feat.fit_transform(X_train, y_train)\n",
    "X_test_autofeat = auto_feat.transform(X_test)\n",
    "\n",
    "# Number of New Features Created\n",
    "print(f\"AutoFeat generated {X_train_autofeat.shape[1] - X_train.shape[1]} new features.\")\n",
    "\n",
    "###############################################################################\n",
    "# 3. Train a RandomForest Regressor with AutoFeat Features\n",
    "###############################################################################\n",
    "auto_feat_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "auto_feat_model.fit(X_train_autofeat, y_train)\n",
    "\n",
    "# Predict and Evaluate\n",
    "y_pred_autofeat = auto_feat_model.predict(X_test_autofeat)\n",
    "\n",
    "auto_feat_results = {\n",
    "    \"R²\": r2_score(y_test, y_pred_autofeat),\n",
    "    \"RMSE\": np.sqrt(mean_squared_error(y_test, y_pred_autofeat)),\n",
    "    \"MAE\": mean_absolute_error(y_test, y_pred_autofeat),\n",
    "}\n",
    "\n",
    "print(\"\\n=== AutoFeat Model Performance ===\")\n",
    "print(auto_feat_results)\n",
    "\n",
    "###############################################################################\n",
    "# 4. Load Semi-Automated Feature Engineering Results\n",
    "###############################################################################\n",
    "# These values should be replaced with actual results from the semi-automated process\n",
    "print(\"\\nLoading Results from Semi-Automated Feature Engineering...\")\n",
    "semi_auto_results = {\n",
    "    \"R²\": 0.5894,  \n",
    "    \"RMSE\": 200.03,\n",
    "    \"MAE\": 134.65\n",
    "}\n",
    "\n",
    "print(\"\\n=== Semi-Automated Feature Engineering Performance ===\")\n",
    "print(semi_auto_results)\n",
    "\n",
    "###############################################################################\n",
    "# 5. Compare Semi-Automated vs. AutoFeat\n",
    "###############################################################################\n",
    "print(\"\\nComparison Between Semi-Automated and AutoFeat (Amsterdam)\")\n",
    "print(\"Semi-Automated:\", semi_auto_results)\n",
    "print(\"AutoFeat:\", auto_feat_results)\n",
    "\n",
    "# Convert dictionary values to arrays for comparison\n",
    "semi_auto_scores = np.array(list(semi_auto_results.values()))\n",
    "auto_feat_scores = np.array(list(auto_feat_results.values()))\n",
    "\n",
    "# Ensure both arrays have the same shape before performing the t-test\n",
    "valid_mask = ~np.isnan(semi_auto_scores) & ~np.isnan(auto_feat_scores)\n",
    "semi_auto_valid = semi_auto_scores[valid_mask]\n",
    "auto_feat_valid = auto_feat_scores[valid_mask]\n",
    "\n",
    "# Run Paired T-Test only if at least 2 valid metrics exist\n",
    "if len(semi_auto_valid) > 1 and len(auto_feat_valid) > 1:\n",
    "    t_stat, p_value = ttest_rel(semi_auto_valid, auto_feat_valid)\n",
    "\n",
    "    print(f\"\\nPaired T-Test (Semi-Automated vs. AutoFeat): t={t_stat:.4f}, p-value={p_value:.4f}\")\n",
    "\n",
    "    # Conclusion\n",
    "    if p_value < 0.05:\n",
    "        print(\"Statistically significant difference found between Semi-Automated and AutoFeat approach.\")\n",
    "    else:\n",
    "        print(\"No significant difference found between Semi-Automated and AutoFeat approach.\")\n",
    "else:\n",
    "    print(\"Not enough valid metrics to perform the paired t-test.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
